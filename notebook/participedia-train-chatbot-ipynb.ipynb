{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10045478,"sourceType":"datasetVersion","datasetId":6188640},{"sourceId":10088140,"sourceType":"datasetVersion","datasetId":6220209}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Preparation\nClean the datasets in the Kaggle environment before embedding generation and model training.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Dataset paths\ncases_path = \"/kaggle/input/participedia-ai-chatbot-project/Participedia_project/data/data_cases.csv\"\nmethods_path = \"/kaggle/input/participedia-ai-chatbot-project/Participedia_project/data/data_methods.csv\"\norganizations_path = \"/kaggle/input/participedia-ai-chatbot-project/Participedia_project/data/data_organizations.csv\"\n\n# Load datasets\ncases_df = pd.read_csv(cases_path).dropna(subset=[\"description\"])\nmethods_df = pd.read_csv(methods_path).dropna(subset=[\"description\"])\norganizations_df = pd.read_csv(organizations_path).dropna(subset=[\"description\"])\n\n# Clean text\ndef clean_text(text):\n    \"\"\"Clean text by removing URLs, extra spaces, and unwanted characters.\"\"\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n    text = re.sub(r'[^A-Za-z0-9.,!?\\'\\`]', ' ', text)  # Remove unwanted characters\n    return text.strip()\n\n# Add context validation\ndef validate_context(row):\n    \"\"\"Ensure context is populated; fallback to description or body.\"\"\"\n    if pd.notnull(row.get(\"context\")) and row[\"context\"].strip():\n        return row[\"context\"]\n    elif pd.notnull(row.get(\"description\")):\n        return row[\"description\"]\n    elif pd.notnull(row.get(\"body\")):\n        return row[\"body\"]\n    return \"No context available\"\n\n# Apply cleaning and context validation\nfor df in [cases_df, methods_df, organizations_df]:\n    df[\"description\"] = df[\"description\"].apply(clean_text)\n    df[\"context\"] = df.apply(validate_context, axis=1)\n\n# Save cleaned datasets\ncases_cleaned_path = \"/kaggle/working/cleaned_cases.csv\"\nmethods_cleaned_path = \"/kaggle/working/cleaned_methods.csv\"\norganizations_cleaned_path = \"/kaggle/working/cleaned_organizations.csv\"\n\ncases_df.to_csv(cases_cleaned_path, index=False)\nmethods_df.to_csv(methods_cleaned_path, index=False)\norganizations_df.to_csv(organizations_cleaned_path, index=False)\n\n# Log summary\nprint(f\"Cases dataset cleaned and saved to {cases_cleaned_path}.\")\nprint(f\"Methods dataset cleaned and saved to {methods_cleaned_path}.\")\nprint(f\"Organizations dataset cleaned and saved to {organizations_cleaned_path}.\")\nprint(f\"Total records: Cases ({len(cases_df)}), Methods ({len(methods_df)}), Organizations ({len(organizations_df)}).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T03:49:32.103635Z","iopub.execute_input":"2024-12-06T03:49:32.104100Z","iopub.status.idle":"2024-12-06T03:49:34.139477Z","shell.execute_reply.started":"2024-12-06T03:49:32.104064Z","shell.execute_reply":"2024-12-06T03:49:34.138667Z"}},"outputs":[{"name":"stdout","text":"Cases dataset cleaned and saved to /kaggle/working/cleaned_cases.csv.\nMethods dataset cleaned and saved to /kaggle/working/cleaned_methods.csv.\nOrganizations dataset cleaned and saved to /kaggle/working/cleaned_organizations.csv.\nTotal records: Cases (1964), Methods (351), Organizations (490).\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Generate Embeddings\nCreate embeddings for all three datasets using sentence-transformers.","metadata":{}},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T03:49:59.652915Z","iopub.execute_input":"2024-12-06T03:49:59.653260Z","iopub.status.idle":"2024-12-06T03:50:11.591383Z","shell.execute_reply.started":"2024-12-06T03:49:59.653230Z","shell.execute_reply":"2024-12-06T03:50:11.590351Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport faiss\n\n# Initialize tokenizer and model\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).eval()\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Validation function for descriptions\ndef validate_descriptions(df, column_name):\n    \"\"\"\n    Validates and filters rows with valid descriptions in the DataFrame.\n    \"\"\"\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n    df = df[df[column_name].notnull() & (df[column_name].str.strip() != \"\")]\n    print(f\"Validated {column_name}: {len(df)} rows remaining.\")\n    return df\n\n# Function to generate embeddings\ndef generate_embeddings(texts, tokenizer, model, device):\n    \"\"\"\n    Generates embeddings for a list of texts using a pre-trained model.\n    \"\"\"\n    embeddings = []\n    with torch.no_grad():\n        for text in tqdm(texts, desc=\"Generating Embeddings\"):\n            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n            inputs = {key: val.to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n    return embeddings\n\n# Load cleaned datasets\ncases_df = pd.read_csv(\"/kaggle/working/cleaned_cases.csv\")\nmethods_df = pd.read_csv(\"/kaggle/working/cleaned_methods.csv\")\norganizations_df = pd.read_csv(\"/kaggle/working/cleaned_organizations.csv\")\n\n# Validate datasets\ncases_df = validate_descriptions(cases_df, \"description\")\nmethods_df = validate_descriptions(methods_df, \"description\")\norganizations_df = validate_descriptions(organizations_df, \"description\")\n\n# Generate embeddings\ncases_embeddings = generate_embeddings(cases_df[\"description\"].tolist(), tokenizer, model, device)\nmethods_embeddings = generate_embeddings(methods_df[\"description\"].tolist(), tokenizer, model, device)\norganizations_embeddings = generate_embeddings(organizations_df[\"description\"].tolist(), tokenizer, model, device)\n\n# Save validated DataFrames\ncases_df.to_csv(\"/kaggle/working/validated_cases.csv\", index=False)\nmethods_df.to_csv(\"/kaggle/working/validated_methods.csv\", index=False)\norganizations_df.to_csv(\"/kaggle/working/validated_organizations.csv\", index=False)\n\n# Save embeddings\nwith open(\"/kaggle/working/cases_with_embeddings.pkl\", \"wb\") as f:\n    pickle.dump(cases_embeddings, f)\n\nwith open(\"/kaggle/working/methods_with_embeddings.pkl\", \"wb\") as f:\n    pickle.dump(methods_embeddings, f)\n\nwith open(\"/kaggle/working/organizations_with_embeddings.pkl\", \"wb\") as f:\n    pickle.dump(organizations_embeddings, f)\n\nprint(\"Embeddings generated and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T03:51:10.309120Z","iopub.execute_input":"2024-12-06T03:51:10.309904Z","iopub.status.idle":"2024-12-06T03:51:31.467850Z","shell.execute_reply.started":"2024-12-06T03:51:10.309868Z","shell.execute_reply":"2024-12-06T03:51:31.466878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd5cdf7aa8b4112ac716a29be2879d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0441cc0d8e3d49bc8a288853f4a38f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b06a06a04044939f6bfd4092b94943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a69e9bf85774946badac0a6e686d7d3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb88bb8d40454ec994118a687589cd82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4556b2d3f5584030b21d7935e2a54c6e"}},"metadata":{}},{"name":"stdout","text":"Validated description: 1964 rows remaining.\nValidated description: 351 rows remaining.\nValidated description: 490 rows remaining.\n","output_type":"stream"},{"name":"stderr","text":"Generating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1964/1964 [00:08<00:00, 225.49it/s]\nGenerating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 351/351 [00:01<00:00, 236.79it/s]\nGenerating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 490/490 [00:02<00:00, 242.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Embeddings generated and saved.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Create FAISS Indices\nBuild and save FAISS indices for each dataset.","metadata":{}},{"cell_type":"code","source":"# Convert embeddings to numpy arrays\ncases_embeddings = np.array(cases_embeddings, dtype=\"float32\")\nmethods_embeddings = np.array(methods_embeddings, dtype=\"float32\")\norganizations_embeddings = np.array(organizations_embeddings, dtype=\"float32\")\n\n# Create FAISS indices\ncases_index = faiss.IndexFlatL2(cases_embeddings.shape[1])\nmethods_index = faiss.IndexFlatL2(methods_embeddings.shape[1])\norganizations_index = faiss.IndexFlatL2(organizations_embeddings.shape[1])\n\n# Add embeddings to indices\ncases_index.add(cases_embeddings)\nmethods_index.add(methods_embeddings)\norganizations_index.add(organizations_embeddings)\n\n# Save FAISS indices\nfaiss.write_index(cases_index, \"/kaggle/working/cases_faiss_index\")\nfaiss.write_index(methods_index, \"/kaggle/working/methods_faiss_index\")\nfaiss.write_index(organizations_index, \"/kaggle/working/organizations_faiss_index\")\n\n# Validate FAISS indices\nassert cases_index.ntotal == len(cases_df), \"Mismatch in cases FAISS index.\"\nassert methods_index.ntotal == len(methods_df), \"Mismatch in methods FAISS index.\"\nassert organizations_index.ntotal == len(organizations_df), \"Mismatch in organizations FAISS index.\"\n\nprint(\"FAISS indices created and validated successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T03:55:20.129845Z","iopub.execute_input":"2024-12-06T03:55:20.130188Z","iopub.status.idle":"2024-12-06T03:55:20.150621Z","shell.execute_reply.started":"2024-12-06T03:55:20.130163Z","shell.execute_reply":"2024-12-06T03:55:20.149754Z"}},"outputs":[{"name":"stdout","text":"FAISS indices created and validated successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Retrain Classification Model\nTrain a new model to classify queries into cases, methods, or organizations","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score, precision_score, recall_score  # Add this line\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport numpy as np\nimport pandas as pd\n\n# Combine datasets\ncases_df[\"category\"] = \"cases\"\nmethods_df[\"category\"] = \"methods\"\norganizations_df[\"category\"] = \"organizations\"\ncombined_df = pd.concat([cases_df, methods_df, organizations_df], ignore_index=True)\n\n# Prepare data\ntexts = combined_df[\"description\"].tolist()\nlabels = combined_df[\"category\"].map({\"cases\": 0, \"methods\": 1, \"organizations\": 2}).tolist()\n\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    texts, labels, test_size=0.2, stratify=labels, random_state=42\n)\n\n# Compute class weights for imbalanced data\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Tokenize\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n\n# Create Dataset class\nclass ParticipediaDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = ParticipediaDataset(train_encodings, train_labels)\ntest_dataset = ParticipediaDataset(test_encodings, test_labels)\n\n# Train model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Move to GPU if available\n\n# Add class weights to loss function\nmodel.classifier = torch.nn.Linear(model.config.hidden_size, model.config.num_labels, bias=True)\nmodel.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    learning_rate=3e-5,  # Fine-tuned learning rate\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    num_train_epochs=5,  # Increase for better performance\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    save_total_limit=2,\n)\n\n# Evaluation metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\n        \"accuracy\": (predictions == labels).mean(),\n        \"f1\": f1_score(labels, predictions, average=\"weighted\"),  # Requires import\n        \"precision\": precision_score(labels, predictions, average=\"weighted\"),  # Requires import\n        \"recall\": recall_score(labels, predictions, average=\"weighted\"),  # Requires import\n    }\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/classification_model\")\ntokenizer.save_pretrained(\"/kaggle/working/classification_model\")\n\nprint(\"Model trained and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T03:55:29.068042Z","iopub.execute_input":"2024-12-06T03:55:29.068469Z","iopub.status.idle":"2024-12-06T04:01:25.930875Z","shell.execute_reply.started":"2024-12-06T03:55:29.068428Z","shell.execute_reply":"2024-12-06T04:01:25.930007Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d39082ea33441ba8ca7099f9552955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c18d05046154c78abd422b45b843d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b03f07deb6542fb93944738502dc020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e028af23b24deca896f355b9914b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc84d5e5abac4e57a0d23ef2f16ec6f3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113573188888242, max=1.0‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b1079ea5e043ea80faa56298c5bfbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241206_035633-fg6mtn86</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface/runs/fg6mtn86' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface/runs/fg6mtn86' target=\"_blank\">https://wandb.ai/dennisdarko01011990-northeastern-university/huggingface/runs/fg6mtn86</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='705' max='705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [705/705 04:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.264375</td>\n      <td>0.914439</td>\n      <td>0.913650</td>\n      <td>0.913582</td>\n      <td>0.914439</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.322926</td>\n      <td>0.914439</td>\n      <td>0.912582</td>\n      <td>0.913367</td>\n      <td>0.914439</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.398575</td>\n      <td>0.910873</td>\n      <td>0.910873</td>\n      <td>0.910873</td>\n      <td>0.910873</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.186700</td>\n      <td>0.487841</td>\n      <td>0.914439</td>\n      <td>0.913712</td>\n      <td>0.916830</td>\n      <td>0.914439</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.186700</td>\n      <td>0.464814</td>\n      <td>0.914439</td>\n      <td>0.913946</td>\n      <td>0.914305</td>\n      <td>0.914439</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.26437482237815857, 'eval_accuracy': 0.9144385026737968, 'eval_f1': 0.9136501309946704, 'eval_precision': 0.9135816178607338, 'eval_recall': 0.9144385026737968, 'eval_runtime': 1.5565, 'eval_samples_per_second': 360.414, 'eval_steps_per_second': 5.782, 'epoch': 5.0}\nModel trained and saved.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# QA Model ","metadata":{}},{"cell_type":"markdown","source":"# Set Paths and Load Datasets\nFirst, we set the paths to your cleaned datasets and load them.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Dataset paths\ncases_path = \"/kaggle/input/cleaned-data/cleaned_cases.csv\"\nmethods_path = \"/kaggle/input/cleaned-data/cleaned_methods.csv\"\norganizations_path = \"/kaggle/input/cleaned-data/cleaned_organizations.csv\"\n\n# Load datasets\ncases_df = pd.read_csv(cases_path)\nmethods_df = pd.read_csv(methods_path)\norganizations_df = pd.read_csv(organizations_path)\n\nprint(\"Datasets loaded successfully.\")\nprint(f\"Cases: {len(cases_df)} rows\")\nprint(f\"Methods: {len(methods_df)} rows\")\nprint(f\"Organizations: {len(organizations_df)} rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:02:10.460827Z","iopub.execute_input":"2024-12-06T04:02:10.461551Z","iopub.status.idle":"2024-12-06T04:02:11.285502Z","shell.execute_reply.started":"2024-12-06T04:02:10.461517Z","shell.execute_reply":"2024-12-06T04:02:11.284625Z"}},"outputs":[{"name":"stdout","text":"Datasets loaded successfully.\nCases: 1964 rows\nMethods: 351 rows\nOrganizations: 490 rows\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Prepare the Data for QA Model\nCombine all datasets and create a unified format suitable for training a QA model.","metadata":{}},{"cell_type":"code","source":"# Add a category column to each dataset\ncases_df[\"category\"] = \"cases\"\nmethods_df[\"category\"] = \"methods\"\norganizations_df[\"category\"] = \"organizations\"\n\n# Concatenate all datasets\ncombined_df = pd.concat([cases_df, methods_df, organizations_df], ignore_index=True)\n\n# Ensure `description` column is used as context\nqa_data = combined_df[[\"description\", \"category\"]].rename(columns={\"description\": \"context\"})\n\n# Display data sample\nprint(\"QA Data Sample:\")\nprint(qa_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:02:15.732081Z","iopub.execute_input":"2024-12-06T04:02:15.732470Z","iopub.status.idle":"2024-12-06T04:02:15.759262Z","shell.execute_reply.started":"2024-12-06T04:02:15.732441Z","shell.execute_reply":"2024-12-06T04:02:15.758501Z"}},"outputs":[{"name":"stdout","text":"QA Data Sample:\n                                             context category\n0  An independent, non partisan assembly of 160 r...    cases\n1  The Minneapolis Neighborhood Revitalization Pr...    cases\n2  One of China s most innovative forms of reinve...    cases\n3  Citizens' Forum Europe was designed to allow d...    cases\n4  Two deliberative forums involving members of t...    cases\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Train a Fine-Tuned QA Model\nWe will use Hugging Face's transformers library for fine-tuning a pre-trained QA model.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import BertTokenizer, BertForQuestionAnswering, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\n\n# File paths\ncases_path = \"/kaggle/input/cleaned-data/cleaned_cases.csv\"\nmethods_path = \"/kaggle/input/cleaned-data/cleaned_methods.csv\"\norganizations_path = \"/kaggle/input/cleaned-data/cleaned_organizations.csv\"\n\n# Load datasets\ncases_df = pd.read_csv(cases_path)\nmethods_df = pd.read_csv(methods_path)\norganizations_df = pd.read_csv(organizations_path)\n\n# Combine datasets and prepare labels\ncases_df[\"category\"] = \"cases\"\nmethods_df[\"category\"] = \"methods\"\norganizations_df[\"category\"] = \"organizations\"\n\n# Ensure all datasets have consistent column names\ncases_df.rename(columns={\"description\": \"context\"}, inplace=True)\nmethods_df.rename(columns={\"description\": \"context\"}, inplace=True)\norganizations_df.rename(columns={\"description\": \"context\"}, inplace=True)\n\n# Keep only required columns\ncases_df = cases_df[[\"context\", \"category\"]]\nmethods_df = methods_df[[\"context\", \"category\"]]\norganizations_df = organizations_df[[\"context\", \"category\"]]\n\n# Combine all datasets\ncombined_df = pd.concat([cases_df, methods_df, organizations_df], ignore_index=True)\n\n# Map categories to numerical labels\ncategory_map = {\"cases\": 0, \"methods\": 1, \"organizations\": 2}\ncombined_df[\"label\"] = combined_df[\"category\"].map(category_map)\n\n# Ensure there are no missing values in the relevant columns\ncombined_df = combined_df.dropna(subset=[\"context\", \"label\"])\n\n# Split data into training and testing sets\ntrain_df, test_df = train_test_split(\n    combined_df, test_size=0.2, stratify=combined_df[\"label\"], random_state=42\n)\n\n# Ensure that train_df and test_df have \"context\" and \"label\" columns as pandas Series\ntrain_texts = list(train_df[\"context\"].values)\ntest_texts = list(test_df[\"context\"].values)\ntrain_labels = list(train_df[\"label\"].values)\ntest_labels = list(test_df[\"label\"].values)\n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"context\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n\n# Custom Dataset Class\nclass QADataset(Dataset):\n    def __init__(self, texts, labels):\n        # Ensure all inputs are strings\n        self.texts = [str(text) if not isinstance(text, str) else text for text in texts]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Ensure the text is properly formatted\n        encodings = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n        encodings = {key: val.squeeze(0) for key, val in encodings.items()}\n        # Dummy start and end positions for fine-tuning\n        encodings[\"start_positions\"] = torch.tensor(0)  # Dummy start position\n        encodings[\"end_positions\"] = torch.tensor(1)  # Dummy end position\n        return encodings\n\n# Create datasets\ntrain_dataset = QADataset(train_texts, train_labels)\ntest_dataset = QADataset(test_texts, test_labels)\n\nprint(\"Datasets prepared successfully.\")\n\n# Load model\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    save_total_limit=2,\n    logging_steps=50,\n    load_best_model_at_end=True,\n)\n\n# Custom Trainer for QA task\nclass QATrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        start_positions = inputs.pop(\"start_positions\")\n        end_positions = inputs.pop(\"end_positions\")\n        outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs.loss\n        return (loss, outputs) if return_outputs else loss\n\n# Initialize Trainer\ntrainer = QATrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n)\n\n# Train the model\ntrainer.train()\n\nprint(\"Model trained and saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:02:22.158554Z","iopub.execute_input":"2024-12-06T04:02:22.159161Z","iopub.status.idle":"2024-12-06T04:11:33.862927Z","shell.execute_reply.started":"2024-12-06T04:02:22.159128Z","shell.execute_reply":"2024-12-06T04:11:33.862064Z"}},"outputs":[{"name":"stdout","text":"Datasets prepared successfully.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='843' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [843/843 09:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.223100</td>\n      <td>0.001074</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.001700</td>\n      <td>0.000365</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.000900</td>\n      <td>0.000260</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000700</td>\n      <td>0.000205</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.000600</td>\n      <td>0.000167</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.000500</td>\n      <td>0.000146</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.000400</td>\n      <td>0.000127</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.000400</td>\n      <td>0.000115</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.000300</td>\n      <td>0.000106</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000300</td>\n      <td>0.000099</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.000300</td>\n      <td>0.000093</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000300</td>\n      <td>0.000089</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.000300</td>\n      <td>0.000085</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.000300</td>\n      <td>0.000083</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000200</td>\n      <td>0.000081</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.000200</td>\n      <td>0.000080</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model trained and saved successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Evaluate the Model","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\neval_results = trainer.evaluate()\n\n# Print evaluation metrics\nprint(\"Evaluation Results:\")\nfor key, value in eval_results.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:13:44.588043Z","iopub.execute_input":"2024-12-06T04:13:44.588405Z","iopub.status.idle":"2024-12-06T04:13:53.949217Z","shell.execute_reply.started":"2024-12-06T04:13:44.588374Z","shell.execute_reply":"2024-12-06T04:13:53.948475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [71/71 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results:\neval_loss: 9.881587902782485e-05\neval_runtime: 9.3501\neval_samples_per_second: 59.999\neval_steps_per_second: 7.594\nepoch: 3.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Save the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"./qa_model_fine_tuned\")\ntokenizer.save_pretrained(\"./qa_model_fine_tuned\")\n\nprint(\"Model and tokenizer saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:14:18.203498Z","iopub.execute_input":"2024-12-06T04:14:18.203853Z","iopub.status.idle":"2024-12-06T04:14:19.211607Z","shell.execute_reply.started":"2024-12-06T04:14:18.203824Z","shell.execute_reply":"2024-12-06T04:14:19.210620Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved successfully.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}